{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22344,
     "status": "ok",
     "timestamp": 1697690195158,
     "user": {
      "displayName": "Steven",
      "userId": "10893707739290145739"
     },
     "user_tz": -480
    },
    "id": "kBiYZ6b3DEfZ",
    "outputId": "1f860014-d53e-44b5-ea9d-6695a8801941"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import os\n",
    "import numpy as np\n",
    "from segmentation_models_pytorch import Unet\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_label = {0: 'Basophil', 1: 'Eosinophil', 2: 'Lymphocyte', 3: 'Monocyte', 4: 'Neutrophil'}\n",
    "label_to_id = {'Basophil': 0, 'Eosinophil': 1, 'Lymphocyte': 2, 'Monocyte': 3, 'Neutrophil': 4}\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, root_dir, mask_dir, final_transform, mask_transform, color_transform, resize):\n",
    "        self.root_dir = root_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.final_transform = final_transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.color_transform = color_transform\n",
    "        self.images = []\n",
    "        self.targets = []\n",
    "        self.resize = resize\n",
    "\n",
    "        for subdir in os.listdir(root_dir):\n",
    "            if os.path.isdir(os.path.join(root_dir, subdir)):\n",
    "                for filename in os.listdir(os.path.join(root_dir, subdir)):\n",
    "                    if filename.endswith(\".jpg\"):\n",
    "                        image_path = os.path.join(root_dir, subdir, filename)\n",
    "                        mask_path = os.path.join(mask_dir, subdir, filename)\n",
    "                        if os.path.exists(mask_path):\n",
    "                            self.images.append((image_path, mask_path))\n",
    "                        else:\n",
    "                            # If there's no mask, create a white mask\n",
    "                            self.images.append((image_path, None))\n",
    "                        self.targets.append(label_to_id[os.path.basename(os.path.dirname(image_path))])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, mask_path = self.images[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if mask_path is not None:\n",
    "            mask = Image.open(mask_path).convert(\"L\")\n",
    "        else:\n",
    "            mask = Image.new(\"L\", (self.resize, self.resize), 255)\n",
    "\n",
    "        image = transforms.ToTensor()(image)\n",
    "        mask = transforms.ToTensor()(mask)\n",
    "        seed = torch.randint(2147483647, (1,)).item()\n",
    "        image = transforms.Resize(int(self.resize))(image)\n",
    "        mask = transforms.Resize(int(self.resize))(mask)\n",
    "        torch.manual_seed(seed)\n",
    "        image = self.mask_transform(image)\n",
    "        torch.manual_seed(seed)\n",
    "        mask = self.mask_transform(mask)\n",
    "        if self.color_transform != None:\n",
    "            image = self.color_transform(image)\n",
    "        image = torch.cat((image, mask), dim=0)\n",
    "        image = self.final_transform(image)\n",
    "\n",
    "        label = label_to_id[os.path.basename(os.path.dirname(image_path))]\n",
    "        return image, label\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, root_dir, mask_dir, final_transform, resize):\n",
    "        self.root_dir = root_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.final_transform = final_transform\n",
    "        self.images = []\n",
    "        self.resize = resize\n",
    "\n",
    "        for subdir in os.listdir(root_dir):\n",
    "            if os.path.isdir(os.path.join(root_dir, subdir)):\n",
    "                for filename in os.listdir(os.path.join(root_dir, subdir)):\n",
    "                    if filename.endswith(\".jpg\"):\n",
    "                        image_path = os.path.join(root_dir, subdir, filename)\n",
    "                        mask_path = os.path.join(mask_dir, subdir, filename)\n",
    "                        if os.path.exists(mask_path):\n",
    "                            self.images.append((image_path, mask_path))\n",
    "                        else:\n",
    "                            self.images.append((image_path, None))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, mask_path = self.images[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if mask_path is not None:\n",
    "            mask = Image.open(mask_path).convert(\"L\")\n",
    "        else:\n",
    "            mask = Image.new(\"L\", (self.resize, self.resize), 255)\n",
    "\n",
    "        seed = torch.randint(2147483647, (1,)).item()\n",
    "        image = transforms.ToTensor()(image)\n",
    "        mask = transforms.ToTensor()(mask)\n",
    "        image = transforms.Resize(self.resize)(image)\n",
    "        \n",
    "        image = torch.cat((image, mask), dim=0)\n",
    "        image = self.final_transform(image)\n",
    "\n",
    "        label = os.path.basename(os.path.dirname(image_path))\n",
    "        label = label_to_id[label]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "aRJDEBpPDY6p"
   },
   "outputs": [],
   "source": [
    "def init(porportion, resize):    \n",
    "    WBC_1_train_dir = 'WBC_1/train/data'\n",
    "    WBC_1_train_mask_dir = 'WBC_1/train/mask'\n",
    "    WBC_1_train_pred_mask_dir = 'WBC_1/train/pred_mask'\n",
    "    WBC_10_train_dir = 'WBC_10/train/data'\n",
    "    WBC_10_train_mask_dir = 'WBC_10/train/mask'\n",
    "    WBC_10_train_pred_mask_dir = 'WBC_10/train/pred_mask'\n",
    "    WBC_50_train_dir = 'WBC_50/train/data'\n",
    "    WBC_50_train_mask_dir = 'WBC_50/train/mask'\n",
    "    WBC_50_train_pred_mask_dir = 'WBC_50/train/pred_mask'\n",
    "    WBC_100_train_dir = 'WBC_100/train/data'\n",
    "    WBC_100_train_mask_dir = 'WBC_100/train/mask'\n",
    "    WBC_100_train_pred_mask_dir = 'WBC_100/train/pred_mask'\n",
    "    WBC_100_val_dir = 'WBC_100/val/data'\n",
    "    WBC_100_mask_dir = 'WBC_100/val/mask'\n",
    "    CAM16_100_train_dir = 'CAM16_100cls_10mask/train/data'\n",
    "    CAM16_100_train_mask_dir = 'CAM16_100cls_10mask/train/mask'\n",
    "    CAM16_100_val_dir = 'CAM16_100cls_10mask/val/data'\n",
    "    CAM16_100_test_dir = 'CAM16_100cls_10mask/test/data'\n",
    "\n",
    "\n",
    "    WBC_train_dir = 'WBC_' + str(proportion) + '/train/data'\n",
    "    WBC_mask_dir = 'WBC_' + str(proportion) + '/train/mask'\n",
    "    WBC_pred_mask_dir ='WBC_' + str(proportion) + '/train/pred_mask'\n",
    "\n",
    "    \n",
    "\n",
    "    final_transform = transforms.Compose([\n",
    "        transforms.Resize((resize, resize)),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406, 0.0], [0.229, 0.224, 0.225, 1.0])\n",
    "    ])\n",
    "\n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(resize),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ])\n",
    "    \n",
    "    color_transform = transforms.Compose([\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    ])\n",
    "\n",
    "    train_dataset = TrainDataset(root_dir=WBC_train_dir, mask_dir=WBC_pred_mask_dir, \n",
    "                                 final_transform=final_transform, mask_transform=mask_transform,\n",
    "                                 color_transform=color_transform, resize=resize)\n",
    "    val_dataset = TestDataset(root_dir=WBC_100_val_dir, mask_dir=WBC_100_mask_dir, \n",
    "                              final_transform=final_transform, resize=resize)\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # if device == torch.device(\"cuda\"):\n",
    "    #     train_dataset = [(x.to(device), torch.tensor([y]).to(device)) for x, y in train_dataset]\n",
    "    #     val_dataset = [(x.to(device), torch.tensor([y]).to(device)) for x, y in val_dataset]\n",
    "\n",
    "    batch_size = 32\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    class_counts = []\n",
    "    label_counts = Counter(train_dataset.targets)\n",
    "    for label, count in label_counts.items():\n",
    "        class_counts.append(count)\n",
    "    class_weights = [1.0 / count for count in class_counts]\n",
    "    \n",
    "    return train_dataloader, val_dataloader, class_weights, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 91536,
     "status": "ok",
     "timestamp": 1697691165287,
     "user": {
      "displayName": "Steven",
      "userId": "10893707739290145739"
     },
     "user_tz": -480
    },
    "id": "el4DHCPADacg",
    "outputId": "7b522e4b-6485-4406-8b31-1f95c30fe532",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(train_dataloader, val_dataloader, class_weights, model_load_path, model_save_path, device):\n",
    "    model = models.resnet34(weights=None)\n",
    "    \n",
    "    # modify the model\n",
    "    new_conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    with torch.no_grad():\n",
    "        new_conv1.weight[:, :3, :, :] = model.conv1.weight\n",
    "        new_conv1.weight[:, 3, :, :] = 0.0\n",
    "    model.conv1 = new_conv1\n",
    "    model.fc = nn.Linear(model.fc.in_features, 5)\n",
    "\n",
    "#     criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights)).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    lr = 0.005\n",
    "    epochs = int(500 / proportion)\n",
    "    opt = optim.SGD(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "    sch = optim.lr_scheduler.StepLR(opt, int(epochs / 5), 0.5)\n",
    "\n",
    "\n",
    "    if model_load_path != \"\":\n",
    "        model.load_state_dict(torch.load(model_load_path))\n",
    "\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        model = model.to(device)\n",
    "\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        io_time = 0\n",
    "        train_time = 0\n",
    "        eval_time = 0\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            io_start_time = time.time()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            io_time += time.time() - io_start_time\n",
    "            opt.zero_grad()\n",
    "            pred = model(inputs)\n",
    "            loss = criterion(pred, labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(pred, 1)\n",
    "            total += len(labels)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        sch.step()\n",
    "        train_time = time.time() - start_time\n",
    "        start_time = time.time()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_dataloader)}')\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Train Accuracy: {accuracy:.2f}%')\n",
    "        losses.append(running_loss / len(train_dataloader))\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_dataloader:\n",
    "                io_start_time = time.time()\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                io_time += time.time() - io_start_time\n",
    "                pred = model(inputs)\n",
    "                _, predicted = torch.max(pred, 1)\n",
    "                total += len(labels)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "    #             print(predicted.shape, labels.shape, total, correct)\n",
    "\n",
    "        eval_time = time.time() - start_time\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Validation Accuracy: {accuracy:.2f}%')\n",
    "        print('IO:', io_time, 'Train:', train_time, 'Eval:', eval_time)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))\n",
    "\n",
    "    ax1.plot(range(epochs), losses, label='Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training Loss Over Epochs')\n",
    "\n",
    "    ax2.plot(range(epochs), accuracies, label='Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Validation Accuracy Over Epochs')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for images, labels in val_dataloader:\n",
    "\n",
    "#     for i in range(len(images)):\n",
    "#         image = images[i, :3] \n",
    "#         mask = images[i, 3] \n",
    "#         label = labels[i]\n",
    "#         print(f\"Label: {label.item()}\")\n",
    "\n",
    "#         plt.imshow(image.permute(1, 2, 0))\n",
    "#         plt.title(\"Original Image\")\n",
    "#         plt.show()\n",
    "\n",
    "#         plt.imshow(mask, cmap='gray')\n",
    "#         plt.title(\"Mask\")\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "proportion = 100\n",
    "resize = 256\n",
    "model_load_path = \"\"\n",
    "model_save_path = \"ResNet34_WBC\" + str(proportion) +\"_model.pth\"\n",
    "train_dataloader, val_dataloader, class_weights, device = init(proportion, resize)\n",
    "train(train_dataloader, val_dataloader, class_weights, model_load_path, model_save_path, device)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPSxnuBKycS8Fff3BNmY1Wk",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:cs5562]",
   "language": "python",
   "name": "conda-env-cs5562-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
